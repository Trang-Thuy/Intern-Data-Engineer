# Báo cáo tuần 7

> ### Bổ sung kiến thức, phần tìm hiểu về BigData, Hadoop, Spark 
#### 1. Giải thích các định nghĩa _Block repeat heartbeat, write editLog_ trong ảnh

![Resolving the Failure Issue of NameNode - Knoldus Blogs](https://blog.knoldus.com/wp-content/uploads/2017/06/namenode.png)

**a. Block Report Heartbeat**
Định kỳ, các DataNode gửi heartbeat đến NameNode để thông báo nó vẫn hoạt động. 
- Heartbeat chỉ ra rằng DataNode vẫn đang hoạt động.
- BlockReport chứa thông tin như ID của block, độ dài block, vị trí block của block được lưu trữ trên DataNode. Thông tin này được NameNode sử dụng để theo dõi các block lưu trữ trong cụm

**b. Write/Read editLog**
Metadata trong NameNode được lưu trữ dưới dạng 2 file:
- FSImage: lưu trữ metadata của NameNode từ khi khởi tạo
- EditLogs: chứa thông tin thay đổi gần nhất của cụm chưa có trong FSImage

Hai file này rất quan trọng trong kiến trúc của HDFS. NameNode được cấu hình để lưu nhiều bản ghi của hai file này cùng một lúc, các cập nhật đều được đồng bộ đến các bản sao này. 

> Nguồn: [Learning Journal: Hadoop Architecture Part -2](https://www.learningjournal.guru/courses/hadoop/hadoop-foundation-training/hadoop-architecture-part-2/) & [SaturnCloud: How to Get Block Report from One Specific Rack in Hadoop](https://saturncloud.io/blog/how-to-get-block-report-from-one-specific-rack-in-hadoop/#:~:text=A%20block%20report%20is%20a,the%20location%20of%20the%20block.)

#### 2. Giải thích ảnh hưởng của Block size đến tốc độ MapReduce
Theo lý thuyết, Các DataNode trong HDFS lưu trữ dữ liệu dưới dạng các block, mỗi block có giá trị mặc định là 128MB. 
Tuy nhiên theo em tìm hiểu, nếu tăng kích thước của block trên HDFS có thể làm tăng tốc độ MapReduce. 
- Nếu tăng kích thước của block, số lượng block của file ít hơn, dẫn đến kích thước của metadata trong NameNode giảm, tương đương với việc lượng công việc của NameNode giảm xuống. Đối với các hệ thống file dữ liệu lớn, đây là một sự cải thiện đáng kể
- Ngoài ra, với số lượng block ít hơn, quá trình lập lịch cho tác vụ đơn giản hơn, nên lượng công việc của Scheduler (trong MapReduce) cũng giảm
> Khi tăng kích thước block, với các file có kích thước là 512MB, 1GB, 1.5GB và 2GB, thời gian thực hiện MapReduce cải thiện (giảm) 2.9%

Tuy nhiên, nếu cân nhắc đến hiệu suất của hệ thống Hadoop thì kích thước block có những ảnh hưởng sau:
- Kích thước block càng nhỏ, thời gian tìm kiếm block dữ liệu càng lâu
- Kích thước block quá lớn, mức độ song song của dữ liệu sẽ giảm, hiệu quả xử lý dữ liệu phân tán giảm

Vì vậy, kích thước của block trong HDFS cần được tối ưu. Hiện nay kích thước mặc định của block là 128MB


> Nguồn: [The Effect Analysis of Block Size in HDFS Againts MapReduce Speed Process](https://www.academia.edu/14636656/The_Effect_Analysis_of_Block_Size_in_HDFS_Againts_MapReduce_Speed_Process) & [Effects of Design Factors of HDFS on I/O Performance](https://thescipub.com/pdf/jcssp.2018.304.309.pdf)

#### 3. Rack là gì

HDFS là một hệ thống file phân tán, ta cần kết nối nhiều máy tính lại và tạo thành một cụm
Rack là khái niệm tập hợp các máy tính (khoảng 30 đến 40 máy) trong cùng hệ thống mạng, sử dụng chung network switch.

Các đặc điểm của rack:
- Các máy tính này thường có tốc độ kết nối với nhau nhanh hơn so với kết nối đến các máy bên ngoài Rack: Rack được cấp một nguồn điện và bộ chuyển mạch chuyên dụng. Nếu công tắc bị lỗi hoặc nguồn điện cung cấp cho một rack có vấn đề, tất cả máy tính thuộc rack đó có thể bị mất kết nối. Hệ thống HDFS có nhiều rack, mỗi rack có công tắc riêng, các công tắc này được kết nối với một công tắc chính. 
- Về phân bố của các block trên DataNode thuộc Rack: Sau khi sao chép các bản ghi của block, HDFS thực hiện ghi block đầu tiên vào DataNode đầu tiên trên một máy, 2 bản sao còn lại sẽ ghi vào 2 DataNode thuộc Rack khác với Rack ban đầu. 
- Thuật toán Rack Awareness giúp nâng cao khả năng chịu lỗi của HDFS. Giả sử, trong trường hợp có một DataNode lỗi thì vẫn còn 2 DataNode khác sẵn sàng. Hoặc trường hợp tất cả các DataNode của cùng một Rack chết thì vẫn còn DataNode ở Rack khác sẵn sàng. Ngoài ra, điều này cũng có lợi cho việc đọc/ghi file.

> Nguồn: [Hadoop Apach: HDFS Architecture](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html?fbclid=IwAR0d2KYg5jV_hJUv5i8ghVAJe-UVX2E4Qbhy-5wVIfNeOUYERW-W3aXvhVE)

#### 4. Nếu cần phải gom đủ tối đa 128MB dữ liệu, dữ  liệu mới được ghi xuống block, tính khả năng chịu lỗi có được đảm bảo không?
Chưa confirm
#### 5. DataNode sẽ thông báo đến người dùng rằng đã ghi dữ thành công trước hay sau khi hoàn tất quá trình tạo các bản sao?

![HDFS Tutorial: Read & Write  Commands using Java API](https://www.guru99.com/images/Big_Data/061114_0923_LearnHDFSAB2.png)
_Quá trình ghi file:_ HDFS dùng để lưu trữ dữ liệu lớn, vì vậy việc đọc ghi file là một tiến trình thường xuyên xảy ra trong công cụ này. Các yêu cầu đọc ghi từ máy khách (client) đều phải đi qua NameNode, tại đây NameNode sẽ tiến hành điều phối cg việc, giao các tác vụ cho DataName. Cơ chế đọc ghi trong HDFS là "Read Once - Write Many", người dùng có thể viết file một lần và đọc nhiều lần. Người dùng không thể chỉnh sửa file mà chỉ có thể append thêm nội dung vào file. 

Khi máy khách (client) submit một request ghi file, file đó sẽ được chia thành các block nhỏ (có kích thước mặc định là 128MB). Quy trình ghi file trong HDFS tuân theo giao thức sau:
- Bước 1 &2: Client gửi yêu cầu ghi file đến NameNode, NameNode kiểm tra quyền và trả lại Client danh sách DataNote mà Client sẽ ghi các block này
- Việc lựa chọn DataNode là tùy thuộc vào tính khả dụng, số lượng replicate và RackAwareness là do NameNode quyết định
- Khi Client nhận được thông tin về danh sách DataNode, việc ghi sẽ tiến hành theo các bước:
	- Bước 3 - 7: Chuẩn bị pipeline: Trước khi tiến hành ghi dữ liệu, Client kiểm tra các DataNode trong tình trạng sẵn sàng nhận dữ liệu hay chưa. Để kiểm tra, với mỗi block, Client sẽ kết nối đến các DataNode và kiểm tra. 
	
	>Giả sử với block A được NameNode quyết định ghi vào DataNode 1, 5 và 6. Client tạo một kết nối TCP/IP đến DataNode 1. Client thông báo DataNode1 hãy sẵn sàng cho việc ghi dữ liệu, đồng thời gửi kèm danh sách 2 DataNode tiếp theo mà block A sẽ tạo tạo bản sao và lưu trữ là DataNode 5 và 6. DataNode 1 sau đó sẽ kết nối đến DataNode 5 để thông báo việc ghi dữ liệu. DataNode 5 thông báo đến Data Node 6 về việc ghi dữ liệu. 
	>
	> Sau khi các bước thông báo hoàn tất, lần lượt DataNode 6 phản hồi DataNode 5, DataNode 5 phản hồi DataNode 1, DataNode 1 phản hồi client về việc sẵn sàng ghi dữ liệu. 
		>
	> Nếu có gì bất thường dẫn đến việc tồn tại một DataNode không sẵn sàng, NameNode gửi lại thông tin danh sách DataNode mới cho Client.
	- Bước 8: Đẩy dữ liệu và tạo bản sao: Sau khi hoàn tất bước chuẩn bị pipeline, dữ liệu bắt đầu được ghi từ client đến DataNode đầu tiên, sau đó các DataNode sẽ tự tạo bản sao (replicate) cho nhau
	
	> Trong ví dụ trên, Client ghi dữ liệu vào DataNode 1. DataNode 1 kết nối đến DataNode 5 và ghi dữ liệu vào DataNode 5. DataNode 5 ghi dữ liệu xong kết nối và ghi dữ liệu đến DataNode 6.
	- Bước 9 & 10: Nhận thông tin phản hồi: Sau khi việc đẩy dữ liệu và sao chép hoàn tất, các DataNode sẽ tuần tự gửi acknowlegement về cho client. Khi DataNode đầu tiên được ghi gửi ack cho client, thông báo rằng block đã được ghi thành công, client thông báo cho NameNode, NamNode thông báo metadata về thông tin vị trí block mới được ghi thành công và ngắt kết nối TCP/IP

Việc ghi dữ liệu này được tiến hành song song trên các block. Giả sử file dữ liệu ban đầu có 3 block cần ghi thì cả ba block đều được ghi cùng lúc. 


> Nguồn: [Guru99 - HDFS Tutorial: Architecture, Read & Write Operation using Java API](https://www.guru99.com/learn-hdfs-a-beginners-guide.html?fbclid=IwAR0J8VWZs9xu_4DnsSKt3X2o04zaYl0rOCGT7YMi8i7ljEjhHb7Pbqj62tY#2)
#### 6. Trong hàm _Shuffle_, sau khi map, dữ liệu sẽ được shuffling để tập hợp các key giống nhau về cùng một partition, trường hợp xảy ra out partition

#### 7. MapReduce: kết quả sau khi map, shuffle và sort được lưu ở đâu, như thế nào trước khi Reducer đọc

Giai đoạn map: có đầu vào là các cặp key - value. Dù dữ liệu đầu vào là dữ liệu có cấu trúc hoặc không có cấu trúc, framework sẽ chuyển dữ liệu này thành cặp key-value.  _Kết quả trung gian sẽ được lưu trữ ở một thư mục tạm thời trên đĩa local_. Các thư mục đầu ra này được tạo ra cho từng tác vụ Map, được sử dụng để lưu trữ dữ liệu trung gian là kết quả của tác vụ. Dữ liệu trung gian này bao gồm các cặp key-value, và ở dạng tệp tuần tự (sequence files). Khi tất cả các tác vụ Map hoàn thành, framework sẽ hợp nhất các tệp từ các thư mục đầu ra trên. Dữ liệu sau khi hợp nhất được chuyển đến Reduce để xử lý.

Giai đoạn reduce: có đầu vào là các cặp key-value từ kết quả trung gian. Kết quả đầu ra của giai đoạn này là kết quả cuối cùng. Kết quả đầu ra cuối cùng được lưu trong HDFS
> Nguồn: [Data Flair Training: Hadoop MapReduce Tutorial - A Complete Guide to Mapreduce](https://data-flair.training/blogs/hadoop-mapreduce-tutorial/) & [Saturn Cloud: In Hadoop Where does the Framework Save the Output of the Map Task in a Normal MapReduce Application](https://saturncloud.io/blog/in-hadoop-where-does-the-framework-save-the-output-of-the-map-task-in-a-normal-mapreduce-application/)

#### 8. Tìm hiểu hai cơ chế khởi động lại Resource Manager: bảo vệ và không bảo vệ
ResourceManager (RM) là thành phần trung tâm, chịu trách nhiệm quản lý tài nguyên, lên lịch các ứng dụng chạy trên YARN. Nhằm mục đích duy trì hoạt động trong suốt quá trình khởi động lại Resource Manager, có hai cơ chế khởi động lại RM:
- **Khởi động lại không bảo toàn (Non-work-preserving)**: 
	- RM lưu metadata ứng dụng (ApplicationSubmissionContext) trong pluggable state-store khi máy khách gửi ứng dụng và lưu trạng thái cuối cùng của ứng dụng (trạng thái không thành công/bị hủy/đã hoàn tất) kèm chẩn đoán khi ứng dụng hoàn tất. Ngoài ra, các thông tin xác thực như khóa bảo mật, token để hoạt động trong môi trường an toàn cũng được lưu lại.
	- Khi RM tắt, nếu thông tin (gồm metadata ứng dụng và thông tin đăng nhập) có sẵn trong state-store, thì khi RM khởi động lại, nó có thể nhận các thông tin trên từ state-store và gửi lại ứng dụng
	- Nếu ứng dụng đã hoàn thành (đã thất bại/bị hủy/đã kết thúc) trước khi RM ngừng hoạt động, RM sẽ không gửi lại ứng dụng sau khi khởi động
	- Trong khoảng thời gian RM ngừng hoạt động, NodeManagers và máy khách sẽ tiếp tục thăm dò RM cho đến khi RM xuất hiện. Khi RM xuất hiện, nó sẽ gửi lệnh đồng bộ hóa (re-sync) tới tất cả các NodeManager và Application Master mà nó giao tiếp thông qua heartbeat. 
	- Các NodeManager sẽ hủy tất cả các container mà nó quản lý, đăng ký lại với RM
	- Khi nhận được lệnh đồng bộ hóa từ RM, các Application Master sẽ tắt. Sau khi RM khởi động lại và tải tất cả các metadata ứng dụng, thông tin xác thực từ state-store và đưa vào bộ nhớ, RM sẽ tạo một Application Master mới cho mỗi ứng dụng chưa hoàn thành, và khởi động lại ứng dụng đó.
	- Như vậy, các tác vụ của ứng dụng đang chạy (trước khi RM dừng hoạt động) sẽ bị mất do RM sử dụng lệnh đồng bộ hóa khi khởi động lại.
	
- **Khởi động lại bảo toàn (Work-preserving)**: Ngược lại với cơ chế khởi động không bảo toàn, trong cơ chế này, RM đảm bảo tính ổn định của trạng thái ứng dụng và tải lại trạng thái đó khi khởi động. Yếu tố được tập trung chủ yếu là việc xây dựng lại toàn bộ trạng thái đang chạy trong cụm YARN, phần lớn trong các trạng thái đó là trạng thái của Scheduler trung tâm trông RM theo dõi tất cả vòng đời của bộ chứa, khoảng trống và yêu cầu tài nguyên của ứng dụng... 
	- RM không cần phải tắt Application Master và chạy lại các ứng dụng từ đầu như trên. Các ứng dụng chỉ cần đống bộ lại với RM và tiếp tục trạng thái trước đó
	- RM khôi phục trạng thái hoạt động bằng cách tận dụng ưu điểm của trạng thái container được gửi từ các NodeManager. NodeManager sẽ không hủy container khi nó đồng bộ lại với RM đã khởi động lại. NodeManager tiếp tục quản lý các container và gửi trạng thái container tới RM khi đăng ký. RM xây dựng lại các phiên bản container và trạng thái lập lịch của các ứng dụng liên quan bằng cách ghi nhận thông tin của container. Trong thời gian chờ khởi động lại, Application Master gửi lại các yêu cầu tài nguyên chưa được phân cho RM vì trong lúc tắt, RM có thể mất các yêu cầu tài nguyên chưa được thực hiện (việc gửi lại yêu cầu do thư viện tự động xử lý)
	
> Nguồn: [Apache Hadoop: Resource Manager Restart](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ResourceManagerRestart.html)

#### 9. Resource Manager đồng bộ StandbyResource Manager như thế nào

Kiến trúc ResourceManager High Availability

![Overview of ResourceManager High Availability](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/images/rm-ha-overview.png)

Để tránh trường hợp single point of failure trong cụm YARN, High Availability được triển khai. Một RM khác chạy song song tại một nút khác được gọi là Standby RM.
Ý tưởng: khi Active RM không hoạt động, Standby RM sẽ hoạt động, đảm bảo hoạt động trơn tru trên cụm
Có hai các để chuyển đổi RM: chuyển đổi thủ công và chuyển đổi tự động 
- Chuyển đổi thủ công và fail-over: Khi tính năng chuyển đổi dự phòng không bật, ta cần chuyển đổi thủ công một trong các Standby RM sang thành Active RM. Để thực hiện việc chuyển đổi dự phòng, đầu tiên cần chuyển Active RM sang chế độ Standby và chuyển StandBy RM sang chế độ Active. Hoạt động này được thực hiện bằng cách sử dụng "yarn rmadmin" CLI. Chuyển đổi theo kiểu thủ công khá tốn kém và không được khuyến khích lựa chọn
- Chuyển đổi tự động: Để quyết định xem RM nào được họn làm Active RM, RM có tùy chọn nhúng ActiveStandbyElector dựa trên Zookeeper. Khi Active RM hiện tại dừng hoạt động hoặc không phản hồi, RM khác sẽ được chọn một cách tự động thành Active RM và sau đó tiếp quản công việc. Tuy nhiên cần đảm bảo trường hợp Split-brain không xảy ra trong chuyển đổi tự động
> Split-brain: Tính huống có nhiều RM đảm nhận vai trò Active RM. Nhiều RM có thể tiến hành quản lý đồng thời môt số nút của cụm, dẫn đến sự không đồng nhất trong quản lý tài nguyên và lập kế hoạch công việc, có khả năng dẫn đến lỗi toàn bộ hệ thống

>Nguồn: [Apache Hadoop: ResourceManager High Availability](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html)
#### 10. YARN cluster: Liệt kê & phân tích ưu điểm, nhược điểm của Cluster mode - Client node 

> Nguồn: [Spark By Examples: Spark Deploy Modes - Client vs. Cluster Explained](https://sparkbyexamples.com/spark/spark-deploy-modes-client-vs-cluster/)
> [Apache Spark: Running Spark on YARN](https://spark.apache.org/docs/latest/running-on-yarn.html#:~:text=In%20cluster%20mode%2C%20the%20Spark,for%20requesting%20resources%20from%20YARN.)
> [Knoldus: Cluster vs. Client: Execution modes for a Spark application](https://blog.knoldus.com/cluster-vs-client-execution-modes-for-a-spark-application/#:~:text=In%20client%20mode%2C%20the%20driver,sense%20to%20use%20cluster%20mode.)
#### 11. Nêu các phép chỉ có thể thực hiện trên Dataset, không thực hiện trong DataFrame
#### 12. Tìm hiểu về định dạng (format) của narrow, wide (shuffles) trao đổi giữa các cặp key-value
#### 14. Có bao nhiêu replicate ở ddaau
#### 15. Trình tự đọc ghi và thứ tự ưu tiên đọc/ghi (data localogta)
#### 16. Tìm hiểu workflow của NameNode High Availability và YARN

