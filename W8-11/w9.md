# Báo cáo tuần 9
## Yêu cầu
### 1. Setup môi trường cài đặt cụm HDFS, YARN
- Yêu cầu chạy được chương trình word count với hadoop mapreduce
- Lưu ý về phiên bản hadoop, spark. Thực tập sinh cần tìm hiểu sự khác nhau giữa các phiên bản
- Cài đặt hadoop 2.x theo mô hình sau (chú ý cài cơ bản k cần HA): https://github.com/longpt233/shopee-analysis/blob/main/imgs-md/hadoop-spark.png
- Link port: Để tránh trùng port các sinh viên dùng port theo công thức: port = port_default + so_thu_tu mình trên sheet. (phần port chỉ định này chưa hoàn thiện,
sẽ gửi lại sau khi tới thời gian )
### 2. Cài đặt Spark standalone, spark trên yarn
- Yêu cầu chạy được chương trình word count với spark chạy trên yarn (tham khảo https://github.com/longpt233/shopee-analysis/blob/main/imgs-
md/hadoop-spark.png)
- Tự sinh dữ liệu người dùng , visualize lên bằng pyspark.
yêu cầu cụ thể:
	- (1) sinh file parquet trên hệ thống hdfs đã cài đặt: khoảng 1 triệu bản ghi gồm các cột: tên, ngày sinh, địa chỉ (địa chỉ random từ 1-100)
	- (2) chạy jupyter notebook
	- (3) sử dụng pyspark để đọc file trên notebook. chạy spark chế độ standalone(nâng cao: *có thể chạy trên yarn*)
	- (4) visualize: thống kê lượng user theo tuổi (khoảng độ tuổi ví dụ từ 10-20, 20-30 tuổi, ...), giới tính bằng các biểu đồ hợp lí
	- (5) chuyển các code pyspark về code java và submit với spark chạy trên yarn. ở bước này không cần visualize mà chỉ cần hiên thị số liệu.
### 3. Thông tin các công config
STT: port = port default + 4
Server List: ['[10.5.94.234](https://10.5.94.234 "https://10.5.94.234")', '[10.5.92.26](https://10.5.92.26 "https://10.5.92.26")', '[10.5.93.113](https://10.5.93.113 "https://10.5.93.113")']

Master Node: 10.5.94.234
Check health: 
- http://10.5.94.234:50074/dfshealth.html#tab-overview
- 10.5.94.234:8092/cluster/

## Nội dung
### 1. Setup môi trường cài đặt cụm HDFS, YARN
#### 1.1. Tìm hiểu sự khác nhau giữa các phiên bản Hadoop
#### 1.2. Cài đặt cụm HDFS, YARN
![a1](https://github.com/Trang-Thuy/Intern-Data-Engineer/assets/82387290/fe1ab4df-e1be-49b5-b2e4-9062f01cf3ac)

**a. Triển khai cài đặt**

Sử dụng phiên bản: Hadoop 2.6.0
Master Node: 10.5.92.234
NameNode: 10.5.93.113 và 10.5.92.26

- Cài đặt theo hướng dẫn: [Akamai: How to Install and Set Up a 3-Node Hadoop Cluster](https://www.linode.com/docs/guides/how-to-install-and-set-up-hadoop-cluster/)
- Sửa đổi các server IP và port theo yêu cầu trong các file config: _core-site.xml, hdfs-site.xml, mapred-site.xml, yarn-site.xml, workers (hoặc slaves)_

**b. Kết quả cài đặt:**

Ở @hdfs4:
![image](https://github.com/Trang-Thuy/Intern-Data-Engineer/assets/82387290/6fe78728-bdd4-4257-ae19-eb81cfcbc20a)
![image](https://github.com/Trang-Thuy/Intern-Data-Engineer/assets/82387290/becafc6f-5abb-428e-8e1e-cded006aa65e)
![image](https://github.com/Trang-Thuy/Intern-Data-Engineer/assets/82387290/94779eb5-8eaa-4826-acb1-023a62d76cec)

Ở @yarn4:
![image](https://github.com/Trang-Thuy/Intern-Data-Engineer/assets/82387290/af43a349-54eb-4097-8118-a8d64523d12e)
![image](https://github.com/Trang-Thuy/Intern-Data-Engineer/assets/82387290/40432581-453a-4a2d-9e2d-5922daa8943c)
![image](https://github.com/Trang-Thuy/Intern-Data-Engineer/assets/82387290/e436cf97-736b-444c-9b6d-2a7295640e89)


#### 1.3. Chạy chương trình WordCount với Hadoop Mapreduce
**a. Triển khai cài đặt**
Cài đặt theo hướng dẫn: [Akamai: How to Install and Set Up a 3-Node Hadoop Cluster](https://www.linode.com/docs/guides/how-to-install-and-set-up-hadoop-cluster/) (mục Put and Get Data to HDFS & Submit MapReduce Jobs to Yarn)

Các bước thực hiện chính:
- Gửi dữ liệu từ máy local/server lên HDFS
- Gửi job MapReduce đến Yarn: Vì gói Hadoop có cung cấp khá nhiều ứng dụng, bao gồm cả ứng dụng word count nên ta có thể sử dụng application của Hadoop cho chương trình

**b. Kết quả**

jpJob completed successfully:
![image](https://github.com/Trang-Thuy/Intern-Data-Engineer/assets/82387290/0608b058-981e-41b2-8a28-7d874d94ff53)

Output:
![image](https://github.com/Trang-Thuy/Intern-Data-Engineer/assets/82387290/35c607fe-5c34-4b85-9427-30516ec867a4)


### 2. Cài đặt Spark standalone, spark trên yarn
#### 2.1. Cài đặt Spark standalone, spark trên yarn
Phiên bản Spark standalone: Spark 3.5.0 
Phiên bản Scala: Scala 2.12.18

#### 2.2. Chạy chương trình Word count  với Spark chạy trên Yarn
**a. Word Count với Spark-submit sử dụng Java**
**b. WordCount với Spark-shell sử dụng Scala**

Cài đặt theo hướng dẫn: [Chương trình Word Count với spark-submit và spark-shell](
 https://demanejar.github.io/posts/word-count-with-spark-submit-and-spark-shell/#wordcount-v%E1%BB%9Bi-spark-shell-s%E1%BB%AD-d%E1%BB%A5ng-scala)
 
 Kết quả:
 Job completed:
 
 ![image](https://github.com/Trang-Thuy/Intern-Data-Engineer/assets/82387290/35251182-2582-48ab-8d2b-3e4b8f7009cd)

 Output:
 ![image](https://github.com/Trang-Thuy/Intern-Data-Engineer/assets/82387290/b067e9be-c1da-47eb-88c6-bee44f3011c0)



#### 2.3. Tạo file dữ liệu, visualize dữ liệu sử dụng Pyspark
Yêu cầu: chạy trên jupyter notebook
Lưu ý: Để sử dụng Jupyter notebook và Pyspark trên server, cần install Anaconda.

**a. Tạo file parquet trên hệ thống hdfs đã cài đặt: khoảng 1 triệu bản ghi gồm các cột: tên, ngày sinh, địa chỉ (địa chỉ random từ 1-100)**
Cài đặt: Sử dụng thư viện/framework faker để sinh dữ liệu
Kết quả:
![image](https://github.com/Trang-Thuy/Intern-Data-Engineer/assets/82387290/a00b5de7-95d2-4ad0-9b94-30d13aed33b0)


**b. Sử dụng pyspark để đọc file trên notebook. chạy spark chế độ standalone (nâng cao: *có thể chạy trên yarn*)**

Kết quả:

![Screenshot from 2023-09-21 17-02-37](https://github.com/Trang-Thuy/Intern-Data-Engineer/assets/82387290/866a7beb-2c03-49ce-bec2-cb09cb9058e9)


**c. Visualize: thống kê lượng user theo tuổi (khoảng độ tuổi ví dụ từ 10-20, 20-30 tuổi, ...), giới tính bằng các biểu đồ hợp lý**

Kết quả:
Phân bố user theo các khoảng độ tuổi:
![image](https://github.com/Trang-Thuy/Intern-Data-Engineer/assets/82387290/eb9fe555-e581-45a4-99a1-f6bf2ecd3e13)

Phân bố user theo giới tính:
![image](https://github.com/Trang-Thuy/Intern-Data-Engineer/assets/82387290/0dbf72b9-63dd-435b-ae62-efbd0c5721ef)

Phân bố giới tính của user theo các khoảng độ tuổi:
![image](https://github.com/Trang-Thuy/Intern-Data-Engineer/assets/82387290/60921987-c905-4f05-882b-fd5f216be689)

**d. Chuyển các code pyspark về code java và submit với spark chạy trên yarn. ở bước này không cần visualize mà chỉ cần hiên thị số liệu**
Cài đặt:
Kết quả:

### 3. Các lưu ý, các lỗi có thể gặp trong quá trình cài đặt
- Lỗi: ```Incompatible clusterIDs in datanode and namenode```

Solve:
``` rm -rf hadoop/data
	cd hadoop/data
	mkdir dataNode nameNode
	cd ../..
	hadoop namenode -format 
```
- Lỗi: ```Exception: Java gateway process exited before sending the driver its port number```

Solve: 
```
export PYSPARK_SUBMIT_ARGS="--master local[2] pyspark-shell"
```
hoặc kiểm tra lại version của Spark (version 3.x is better)

- Lỗi
