# Báo cáo tuần 9
## Yêu cầu
### 1. Setup môi trường cài đặt cụm HDFS, YARN
- Yêu cầu chạy được chương trình word count với hadoop mapreduce
- Lưu ý về phiên bản hadoop, spark. Thực tập sinh cần tìm hiểu sự khác nhau giữa các phiên bản
- Cài đặt hadoop 2.x theo mô hình sau (chú ý cài cơ bản k cần HA): https://github.com/longpt233/shopee-analysis/blob/main/imgs-md/hadoop-spark.png
- Link port: Để tránh trùng port các sinh viên dùng port theo công thức: port = port_default + so_thu_tu mình trên sheet. (phần port chỉ định này chưa hoàn thiện,
sẽ gửi lại sau khi tới thời gian )
### 2. Cài đặt Spark standalone, spark trên yarn
- Yêu cầu chạy được chương trình word count với spark chạy trên yarn (tham khảo https://github.com/longpt233/shopee-analysis/blob/main/imgs-
md/hadoop-spark.png)
- Tự sinh dữ liệu người dùng , visualize lên bằng pyspark.
yêu cầu cụ thể:
	- (1) sinh file parquet trên hệ thống hdfs đã cài đặt: khoảng 1 triệu bản ghi gồm các cột: tên, ngày sinh, địa chỉ (địa chỉ random từ 1-100)
	- (2) chạy jupyter notebook
	- (3) sử dụng pyspark để đọc file trên notebook. chạy spark chế độ standalone(nâng cao: *có thể chạy trên yarn*)
	- (4) visualize: thống kê lượng user theo tuổi (khoảng độ tuổi ví dụ từ 10-20, 20-30 tuổi, ...), giới tính bằng các biểu đồ hợp lí
	- (5) chuyển các code pyspark về code java và submit với spark chạy trên yarn. ở bước này không cần visualize mà chỉ cần hiên thị số liệu.
### 3. Thông tin các công config
STT: port = port default + 4
Server List: ['[10.5.94.234](https://10.5.94.234 "https://10.5.94.234")', '[10.5.92.26](https://10.5.92.26 "https://10.5.92.26")', '[10.5.93.113](https://10.5.93.113 "https://10.5.93.113")']

Master Node: 10.5.94.234
Check health: 
- http://10.5.94.234:50074/dfshealth.html#tab-overview
- 10.5.94.234:8092/cluster/

## Nội dung
### 1. Setup môi trường cài đặt cụm HDFS, YARN
#### 1.1. Tìm hiểu sự khác nhau giữa các phiên bản Hadoop
#### 1.2. Cài đặt cụm HDFS, YARN
**a. Triển khai cài đặt**

Sử dụng phiên bản: Hadoop 2.7
Master Node: 10.5.92.234
NameNode: 10.5.93.113 và 10.5.92.26

- Cài đặt theo hướng dẫn: [Akamai: How to Install and Set Up a 3-Node Hadoop Cluster](https://www.linode.com/docs/guides/how-to-install-and-set-up-hadoop-cluster/)
- Sửa đổi các server IP và port theo yêu cầu trong các file config: _core-site.xml, hdfs-site.xml, mapred-site.xml, yarn-site.xml, workers (hoặc slaves)_

**b. Kết quả cài đặt:**

#### 1.3. Chạy chương trình WordCount với Hadoop Mapreduce
**a. Triển khai cài đặt**
Cài đặt theo hướng dẫn: [Akamai: How to Install and Set Up a 3-Node Hadoop Cluster](https://www.linode.com/docs/guides/how-to-install-and-set-up-hadoop-cluster/) (mục Put and Get Data to HDFS & Submit MapReduce Jobs to Yarn)

Các bước thực hiện chính:
- Gửi dữ liệu từ máy local/server lên HDFS
- Gửi job MapReduce đến Yarn: Vì gói Hadoop có cung cấp khá nhiều ứng dụng, bao gồm cả ứng dụng word count nên ta có thể sử dụng application của Hadoop cho chương trình

**b. Kết quả**

### 2. Cài đặt Spark standalone, spark trên yarn
#### 2.1. Cài đặt Spark standalone, spark trên yarn
Phiên bản Spark standalone: Spark

#### 2.2. Chạy chương trình Word count  với Spark chạy trên Yarn
**a. Word Count với Spark-submit sử dụng Java**
**b. WordCount với Spark-shell sử dụng Scala**

Cài đặt theo hướng dẫn: [Chương trình Word Count với spark-submit và spark-shell](
 https://demanejar.github.io/posts/word-count-with-spark-submit-and-spark-shell/#wordcount-v%E1%BB%9Bi-spark-shell-s%E1%BB%AD-d%E1%BB%A5ng-scala)
 
 Kết quả:
 


#### 2.3. Tạo file dữ liệu, visualize dữ liệu sử dụng Pyspark
Yêu cầu: chạy trên jupyter notebook
Lưu ý: Để sử dụng Jupyter notebook và Pyspark trên server, cần install Anaconda.

**a. Tạo file parquet trên hệ thống hdfs đã cài đặt: khoảng 1 triệu bản ghi gồm các cột: tên, ngày sinh, địa chỉ (địa chỉ random từ 1-100)**
Cài đặt:
Kết quả:

**b. Sử dụng pyspark để đọc file trên notebook. chạy spark chế độ standalone (nâng cao: *có thể chạy trên yarn*)**
Cài đặt:
Kết quả:

**c. Visualize: thống kê lượng user theo tuổi (khoảng độ tuổi ví dụ từ 10-20, 20-30 tuổi, ...), giới tính bằng các biểu đồ hợp lý**
Cài đặt:
Kết quả:

**d. Chuyển các code pyspark về code java và submit với spark chạy trên yarn. ở bước này không cần visualize mà chỉ cần hiên thị số liệu**
Cài đặt:
Kết quả:

### 3. Các lưu ý, các lỗi có thể gặp trong quá trình cài đặt
- Lỗi: ```Incompatible clusterIDs in datanode and namenode```

Solve:
``` rm -rf hadoop/data
	cd hadoop/data
	mkdir dataNode nameNode
	cd ../..
	hadoop namenode -format 
```
- Lỗi: ```Exception: Java gateway process exited before sending the driver its port number```

Solve: 
```
export PYSPARK_SUBMIT_ARGS="--master local[2] pyspark-shell"
```
hoặc kiểm tra lại version của Spark (version 3.x is better)

- Lỗi
